<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Xeon-CafPhi : Caffe deep learning framework - optimized for Xeon Phi">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Xeon-CafPhi</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/rohithj/Xeon-CafPhi">View on GitHub</a>

          <h1 id="project_title">Xeon-CafPhi</h1>
          <h2 id="project_tagline">Caffe deep learning framework - optimized for Xeon Phi</h2>
          <p><h4><a href="#project-proposal">Project Proposal</a> >>
          <a href="#project-checkpoint">Project Checkpoint</a> >>
          <a href="#project-report">Project Report</a></h4></p>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/rohithj/Xeon-CafPhi/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/rohithj/Xeon-CafPhi/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">

<p>Optimize Caffe deep learning framework on <a href="http://ark.intel.com/products/71992/Intel-Xeon-Phi-Coprocessor-5110P-8GB-1_053-GHz-60-core">Xeon Phi</a>. Optimization will be done by evaluating the performance of CIFAR-10 and MNIST data sets. We also plan to make a detailed analysis on the performance of Caffe on Xeon Phi (with and without GPU support).</p>

<hr>


<h2>
<a id="project-report" class="anchor" href="#project-report" aria-hidden="true"><span class="octicon octicon-link"></span></a>Project Report</h2>

<h3>Current Caffe operation</h3>

<pre><code>function train()
  for i=1:num_iterations        // take a batch_size of inputs every input
    forward(net)
    backward(net)
  end
end

function test()
  forward(net)
end
</code></pre>

<p>As can be observed, the caffe neural network algorithm is inherently serial and it’s the forward or backward propagation through different layers that is parallelizable.</p>

<pre><code>function forward(net)
  for i = 1:num_layers-1
    forward(layer(i), layer(i-1))
  end
end
</code></pre>

<p>Then the respective layer’s forward function is called. Since output of previous layer is used in the next layer we must parallelize in the layers only. One such parallelizable layer is the convolutional layer which usually takes up 67% of a neural network’s time.</p>

<p>Up until now, we have <b><strong>successfully changed the matrix multiplication of Caffe to a full convolution as mentioned in the checkpoint</strong></b>.</p>

<p>Our theory was that on any hardware accelerator (XeonPhi or GPU) the matrix multiplication method employed by caffe, would not be able to achieve full speedup using parallelism. <i><em>The reason for this being that they end up memory bound rather than compute bound</em></i>.</p>

<p>The pseudo code currently employed by caffe forward convolution layer is:</p>

<pre><code>For n inputs in this iteration
    Input2D = Im2col(input3D) // change 3D input to a 2D matrix by taking out all 
    //windows of convolution and putting them in colums
    cblas_sgemm(input2D, weight) //Use optimized Cblas Sgemm to multiply input 2D with weight matrix
end
</code></pre>

<p>The extra memory consumed storing Input2D is what we believe causes hardware accelerators to be memory bound. This has been even stated by the creator of caffe as a price he paid for using cblas_sgemm. Plus if you went on to perform the above algorithm on different. We have successfully changed this algorithm to a simple straightforward convolution acting on a single input.</p>

<h3>Our algorithm pseudocode:</h3>
<p>Main thread
Add n inputs to work pool:
Wait for threads</p>

<p>Each thread in pool:
Convolve the image using windowing approach</p>

<pre><code>for i in 1..Num_Input_Channels
  for o in 1..Num_Output_Channels
    for h in 1..Image_height
      for w in 1..Image_width
        for x in 1..Kernel_height
          for y in 1..Kernel_width
            output(o, h, w) += input(i, h+x, w+y) * filter(i, o, x, y)
          end
        end
      end
    end
  end
end
</code></pre>

<p><strong>(Highly parallelizable 6 for loops left)</strong></p>

<p>Firstly as each thread merely reads from input and weights, there is “no cache invalidation on different cores”
For output since each thread writes on a different output buffer, there shouldn’t be any invalidation, but still we are planning to put padding in to ensure this. Avoiding invalidation is necessary as each output pixel is written to (no. of input channels x kernel height x kernel width) times. (Even for a small dataset like MNIST this can be of the order of 1250 times)</p>

<p>Our convolution function does not allocate any memory during convolution and uses the input and output buffers directly, we are pretty memory efficient. We expect to use CPU or Xeon-Phi RAM/Cache or GPU shared memory far more efficiently.</p>

<p>We have achieved a speedup on caffe spawning pthreads naively for each input.</p>

<p><img src="https://cloud.githubusercontent.com/assets/5513664/7548314/0c87c96c-f5d2-11e4-80ca-bc2dbbd0968a.JPG" alt="" width="600" height="400"></p>

<h3>Plan Over the Weekend</h3>
<ul>
    <li>We are code ready, our algorithm is working.</li>

    <li>Dhruv will improve performance for CPU and Rohith will try to optimize Xeon-Phi for the caffe.</li>

    <li>CPU and Xeon-Phi has many opportunities for optimizations, like maintaining a thread pool, padding of output arrays, using Cilk, openMP etc. We are currently evaluating this and we are also trying to optimize other regions of code.</li>

    <li>Sunday afternoon we will complete our analysis and put our results in the report.</li>
</ul>

<hr>

<h2>
<a id="project-checkpoint" class="anchor" href="#project-checkpoint" aria-hidden="true"><span class="octicon octicon-link"></span></a>Project Checkpoint</h2>

<p>We have completed the Integration and analysis of Caffe on our local machines. We are on track with the plan but are behind on evaluating the same on Xeon-Phi on Latedays. We recently got access to the same and will evaluate this in priority. Hence the schedule is modified slightly.</p>

<p>We have benchmarked the Caffe with out of the box CPU implementation in the following configuration:
64-bit Ubuntu-14.04 Linux Virtual machine:
RAM: 8192 MB
Processors: 4
Video Memory: 48 MB</p>

<p>MNIST =&gt; 5.5 seconds/100 iterations</p>

<p>CIFAR-10 =&gt; 25.6 seconds/100 iterations</p>

<h3>
<a id="analysis-caffe-implementation-of-convolution-layer" class="anchor" href="#analysis-caffe-implementation-of-convolution-layer" aria-hidden="true"><span class="octicon octicon-link"></span></a>Analysis: Caffe Implementation of Convolution Layer</h3>

<p>Caffe implements the convolution of different filters over images in the form of matrix multiplication, it converts the convolutions into a set of matrix multiplications.</p>

<p>Convolution is usually done using a sliding window approach, it is an operation where the filter is applied to each pixel and its neighbours and we get the response at that pixel by adding the multiplication of the corresponding elements at that pixel.</p>

<p><img src="http://upload.wikimedia.org/wikipedia/commons/6/6a/Convolution_of_box_signal_with_itself2.gif" alt="Convolution in 1-D"></p>

<p><img src="http://upload.wikimedia.org/wikipedia/commons/b/b9/Convolution_of_spiky_function_with_box2.gif" alt="Convolution in 1-D"></p>

<p>Thus it is an iterative operation where we have multiple for loops</p>

<pre><code>for w in 1..W
  for h in 1..H
    for x in 1..K
      for y in 1..K
        output(w, h) += input(w+x, h+y) * filter(x, y)
      end
    end
  end
end

Where the image is WxH size and filter is KXK size
</code></pre>

<p>The implementer of Caffe was not GPU savvy yet he wished to have his convolution run on a GPU and leverage its many cores to achieve a speed-up and train his neural networks faster. Since he couldn’t easily parallelize the for loops easily, he accomplished this by using a utility much like Matlab’s im2col which changes an input 2d array into a set of columns. Each column has the window instance of image and the filter is also represented as columns and then he uses matrix multiplication to get the output.</p>

<p>Of course he pads the image appropriately as per mask size, which you will see if you look at layers in Caffe (5x5 filters have a padding of 2 on the image).</p>

<p>CBlas libraries are already available optimised for the matrix multiplication and then col2im is applied back to the matrix multiplication output and we get the convolved image.</p>

<p>The major drawback is having to put these column images in RAM which costs time to transfer to GPU and even takes up shared space.</p>

<h3>
<a id="our-design" class="anchor" href="#our-design" aria-hidden="true"><span class="octicon octicon-link"></span></a>Our Design</h3>

<p>We will parallelise the for loops which should save space and we wish to do this for latedays’ XeonPhi processor. We wish to implement the convolution iteratively which the author didn’t do.</p>

<p>We will first be implementing convolution serially and then parallelize the same. If the image in (width X Height) with depth D (Eg: RGB), then each location can be represented as patch of (K X K) which can be interpreted as (K X K X D) vector. M filters are then applied to such a patch. This can be accomplished using multiple for loops in the following format:</p>

<pre><code>for w in Width
  for h in Height
    for i in 1..K
      for j in 1..K
        for m in 1..M
          for d in 1..D
            output(w, h, d) += input(w+i, h+j, d) * filter(m, i, j, d)
          end
        end
      end
    end
  end
end
</code></pre>

<p>We intend to parallelize this implementation by using ISPC, openMP and Pthreads and based on analysis determine which combination of choices at appropriate loops give us the best performance for MNIST and CIFAR-10 data sets. This will require detailed analysis and evaluation.</p>

<p>Following references helped us analyze and understand convolution:</p>

<p><a href="https://github.com/Yangqing/caffe/wiki/Convolution-in-Caffe:-a-memo">Convolution in Caffe</a></p>

<p><a href="http://yann.lecun.com/exdb/lenet/">LeNet-5, convolutional neural networks</a></p>

<p><a href="http://www.wikiwand.com/en/Convolution">Convolution in Wikipedia</a></p>

<hr>

<hr>

<h2>
<a id="project-proposal" class="anchor" href="#project-proposal" aria-hidden="true"><span class="octicon octicon-link"></span></a>Project Proposal</h2>

<h3>
<a id="background" class="anchor" href="#background" aria-hidden="true"><span class="octicon octicon-link"></span></a>Background</h3>

<p><img src="http://deeplearning.net/tutorial/_images/mylenet.png" alt=""></p>

<p>A convolutional neural network is currently a very famous Machine Learning algorithm which is used to classify data. It is especially used in computer vision to classify images with high accuracy. A convolutional neural network applied on an image has multiple axes of parallelism at the different layers of the propagation during the training and testing of the network.</p>

<p>Each hidden layer has multiple feature maps, which are produced by convolving respective weights weighed over all the pixels of the previous layer’s feature maps and adding corresponding results. This convolution task is highly parallelisable. </p>

<p>MaxPooling is a task of subsampling the image by taking the average or maximum amongst subparts of the images. This is done in order to avoid any variance. This task of down-sampling images is also highly parallelisable.</p>

<p>Caffe is a framework written in C++ to implement this convolutional neural network, in order to utilise these axes of parallelism available. Caffe takes the convolutions, uses cblas library to turn them into a bunch of matrix multiplications and the uses CudaDNN with Nvidia GPUs to attain faster training speeds.</p>

<h3>
<a id="challenge" class="anchor" href="#challenge" aria-hidden="true"><span class="octicon octicon-link"></span></a>Challenge</h3>

<p>Optimizing Caffe for Xeon Phi is hard as it is already optimized for both generic CPU and GPU. We intend to find specific parts of Caffe implementation that can be optimized to better use the features of Xeon Phi which will be challenging to find. This will require us to understand the existing parallelism support and find new parallelism or fine tune existing support to that of Xeon Phi. </p>

<p>My partner and I have not used or worked with Caffe before and hence we will need to spend time to use and understand the code. Convolution implementation is done by transforming the input data to matrices and computation is done on these. We would need to understand and reimplement this feature to better utilize parallelism of Xeon Phi, which will be challenging.</p>

<h3>
<a id="resources" class="anchor" href="#resources" aria-hidden="true"><span class="octicon octicon-link"></span></a>Resources</h3>

<ul>
<li>Hardware: We will be using Latedays cluster machine that has Xeon Phi co-processor board. We will also be using our laptops to perform initial testing. </li>
<li>Software: Caffe software which comes with BSD 2-Clause license will be our starting point for this project. Test Data sets: CIFAR-10 and MNIST will also be required for evaluation.</li>
</ul>

<h3>
<a id="goalsdeliverables" class="anchor" href="#goalsdeliverables" aria-hidden="true"><span class="octicon octicon-link"></span></a>Goals/Deliverables:</h3>

<ul>
<li><p>Primary goal is to understand and modify the Caffe codebase to best utilize all the resources available in Xeon Phi. We will be evaluating performance with CIFAR-10 and MNIST data sets and comparing the same with default Caffe code run on the same machine.</p></li>
<li><p>Overall, we would like to gain information about performance benefits of optimizing Caffe on a powerful board such as Xeon Phi. We would also like to gain information about the performance drawbacks of running these optimized code on other machines such as our laptops.</p></li>
<li><p>A possible extension is to explore the optimization of Caffe code with GPU support on Latedays cluster.</p></li>
</ul>

<h3>
<a id="platforms" class="anchor" href="#platforms" aria-hidden="true"><span class="octicon octicon-link"></span></a>Platforms</h3>

<p>We will be using the Linux posix threads framework and ISPC framework to parallelise the caffe implementation for the Xeon Phi processor of Latedays cluster. We will also consider OpenMPI to leverage other 14 cores on Latedays cluster.</p>

<h3>
<a id="schedule" class="anchor" href="#schedule" aria-hidden="true"><span class="octicon octicon-link"></span></a>Schedule</h3>

<ul>
<li>Friday, April 10: Setup environment to run Caffe on our personal systems on a VM. Run caffe framework on MNIST and CIFAR10 dataset and gather performance results.</li>
<li>Friday, April 17: Analyse the Caffe code to understand the operations done for MNIST and CIFAR-10 data sets.</li>
<li>Based on analysis find alternative implementation of Matrix multiplication and frame an outline for Convolution implementation.</li>
<li>Friday, April 24: Run Caffe code on Xeon-Phi on latedays and gather performance results.</li>
<li>Implement and run serial implementation of Convolution on Xeon-Phi. We will time the implementation, recognise the the bottlenecks and improve the implementation. We also intend to find other regions of code that can be parallelized.</li>
<li>Friday, May 1: Parallelize the Convolution implementation and measure performance. We intend to evaluate by using combination of ISPC, OpenMP and pthreads wherever applicable and evaluate the performance. </li>
<li>Optimise the implementation keeping in mind both the cache coherence and the bandwidth.</li>
<li>Friday, May 7: We time our implementation, check for correctness and race the caffe CPU vs GPU implementation.</li>
<li>If time permits recognise opportunities of parallelism utilised by GPU matrix multiplication method and  try to combine GPU and CPU features on Latedays cluster.</li>
</ul>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">Xeon-CafPhi maintained by <a href="https://github.com/rohithj">rohithj</a></p>
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
